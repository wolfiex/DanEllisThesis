\gdef \the@ipfilectr {@-1}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces \textbf {The multifunctionality of the MCM.} A chord diagram showing the functionalisation of all species within the MCM v3.3.1. Arc sizes represent what percentage of all functional groups in the MCM mechanism a group contains. Translucent areas of no outwards links represent species with multiples of a certain functional group, of which Alcohols and Ketones have the most. Source: \citep {cover} \relax }}{7}{figure.caption.7}
\contentsline {figure}{\numberline {1.2}{\ignorespaces \textbf {Construction process of a SMILES string.} The example compound is Melatonin. Although this does not exist within the atmosphere, it provides a clear example of the SMILES string methodology. \autoref {fig:st1} is made using SMILES drawer: \citep {SMILESdrawer} \relax }}{9}{figure.caption.8}
\contentsline {figure}{\numberline {1.3}{\ignorespaces \textbf {Constructing a graph from species structure.} (a) shows the maximum number of times an atom occurs for any single species in the MCM. (b) depicts the graph-like chemical structure of {INB1NBCO3}(a product from isoprene). This is a highly processed species stemming from Isoprene, and this makes for a good example of the bond matrix. Finally, a matrix representing the bonds in \ce {INB1NBCO3} is created from the maximum possible occurrence matrix from (a). For simplicity, empty row/column pairs have been removed to produce (c). This matrix will always be symmetrical as the bonds do not have a direction.\relax }}{10}{figure.caption.9}
\contentsline {figure}{\numberline {1.4}{\ignorespaces \textbf {A graph of an MCM subset representing the chemistry within Beijing.} Here colours show the increase of \ce {O-C} ratio as species are oxidised (lighter). All emitted species ultimately tend towards carbon monoxide which is at the centre of the graph. Node clusters symbolise groups of species which react more between themselves and less with others (This graph only represents the mechanism structure).\relax }}{11}{figure.caption.10}
\contentsline {figure}{\numberline {1.5}{\ignorespaces \textbf {Determining the Principal Compnent of a sample dataset.} It can be seen that in a change in axis to follow the first principal component (right), it is possible to explain most of the variation in the samle dataset (left). Source: \citep {pcaim}\relax }}{14}{figure.caption.11}
\contentsline {figure}{\numberline {1.6}{\ignorespaces \textbf {Representing the t-SNE algorithm as a fully connected force graph.} Here each node is attached to every other node. Nodes with a strong relationship are pulled closer together than those with a weaker one.\relax }}{15}{figure.caption.12}
\contentsline {figure}{\numberline {1.7}{\ignorespaces \textbf {An example of how the curse of dimensionality affects the mapping of points a certian distance from eachother.} \relax }}{16}{figure.caption.13}
\contentsline {figure}{\numberline {1.8}{\ignorespaces \textbf {Showing the difference between PCA and t-SNE clustering.} These figures show the clustering of a set of standardized species concentration profiles (c) across two styles of dimensionality reduction: PCA (a) and t-SNE (b). \relax }}{19}{figure.caption.14}
\contentsline {figure}{\numberline {1.9}{\ignorespaces An example autoencoder structure which reduces a 16 dimentional input to 2. Draw with the aid of \citep {drawae}\relax }}{20}{figure.caption.15}
\contentsline {figure}{\numberline {1.10}{\ignorespaces \textbf {Comparing the result of the 2D encoding and decoding of an Ozone-NOx-Methane isopleth.} The original data (a) is reduced to two dimensions and then reconstructed back into 3D. This is done with Principal Component Analysis (b) and an AutoEncoder (c). The original isopleth is created using 300 simulations of different intial conditions: NOx (variable), Methane (variable) and Ozone (constant). These were designed using a latin hypercube and converted into a surface plot using Delaunay triangulation. \relax }}{21}{figure.caption.16}
\contentsline {figure}{\numberline {1.11}{\ignorespaces The process of converting a graph into a vector using Node2Vec. Source:\citep {n2vimg}\relax }}{21}{figure.caption.17}
\contentsline {figure}{\numberline {1.12}{\ignorespaces Calculation of the random walk path. Source:\citep {node2vec}\relax }}{22}{figure.caption.18}
\contentsline {figure}{\numberline {1.13}{\ignorespaces \textbf {An example 4 colour matching} This uses the first implementation of the algorithm mentioned in \autoref {sec:4col}. The greedy approach does not often find the optimum solution, which may result in 5 colours instead. Observable Notebook : \cite {w4colobs}\relax }}{25}{figure.caption.19}
\contentsline {figure}{\numberline {1.14}{\ignorespaces \textbf {A comparison of different clustering methods on a toy dataset.} The plot shows the performance of several vector clustering algorithms in Scikit-Learn. Cluster algorithms are represented across the horizontal axis and several types of datasets are across the vertical. Clustered groups are coloured. Source: \citep {clustereval}\relax }}{26}{figure.caption.20}
\contentsline {figure}{\numberline {1.15}{\ignorespaces \textbf {A decision tree aggrogate from a random forrest plotted with the Epiphyte version of the TreeSurgeon program \citep {forrester}.} The data originates from \cite {iodene} and the imporance of Tempearature (blue), Depth (orange) and Chlorophyll $a$ (green). It is shown that all models create their first split based on the temperature (is it $>$21 degrees). In the case in is (right branch) the sea depth is seen as the most important variable to test (is it deeper than 26m). This sort of split allow us to get a feel for which (if any) properties are dominant in partitioning the data. \relax }}{28}{figure.caption.21}
\contentsline {figure}{\numberline {1.16}{\ignorespaces \textbf {Comparing clusters for all inputs after a reduction to 2 dimensions using Principle Component analysis.} Each graph has undergone several clustering algorithms under a range of parameters. The result with the best silhouette coefficient has been chosen. Colours follow the greedy four colour theorem and are there only to indicate the contrast between cluster boundaries.\relax }}{30}{figure.caption.22}
\contentsline {figure}{\numberline {1.17}{\ignorespaces \textbf {Comparing clusters for all inputs after a reduction to 2 dimensions using an AutoEncoder.} Each graph has undergone several clustering algorithms under a range of parameters. The result with the best silhouette coefficient has been chosen. Colours follow the greedy four colour theorem and are there only to indicate the contrast between cluster boundaries.\relax }}{31}{figure.caption.23}
\contentsline {figure}{\numberline {1.18}{\ignorespaces \textbf {Comparing clusters for all inputs after a reduction to 2 dimensions using t-SNE.} Each graph has undergone several clustering algorithms under a range of parameters. The result with the best silhouette coefficient has been chosen. Colours follow the greedy four colour theorem and are there only to indicate the contrast between cluster boundaries.\relax }}{32}{figure.caption.24}
\contentsline {figure}{\numberline {1.19}{\ignorespaces \textbf {The individual decomposition of clusters in \autoref {fig:pcavis}(c)}This shows that the main difference between the two clusters is the existance of Nitrogen elements within Nitrate and Peroxyacetyl Nitrate (PAN) groups. The table on the right acts as a key for the colours and shows the overall impotance of each feature in separating an item into the various clusters (using an ensemble of decision trees).\relax }}{34}{figure.caption.25}
\contentsline {figure}{\numberline {1.20}{\ignorespaces \textbf {Comparing feature importance for PCA clusters.} Importance ranges are trimmed at 40\% for comparison. Some categories may contain values greater than this. All bars sum to 100\%.\relax }}{37}{figure.caption.29}
\contentsline {figure}{\numberline {1.21}{\ignorespaces \textbf {Comparing feature importance for AE clusters.} Importance ranges are trimmed at 40\% for comparison. Some categories may contain values greater than this. All bars sum to 100\%. \relax }}{38}{figure.caption.30}
\contentsline {figure}{\numberline {1.22}{\ignorespaces \textbf {Comparing feature importance for t-SNE clusters.} Importance ranges are trimmed at 40\% for comparison. Some categories may contain values greater than this. All bars sum to 100\%.\relax }}{39}{figure.caption.31}
\contentsline {figure}{\numberline {1.23}{\ignorespaces \textbf {Comparing individual clusters between MACCS for PCA and t-SNE algorithm output.} The bar chart to the right is the cumelative chart which represents the splits in deciding the cluster a species falls into from \autoref {sec:fsclust}. Unlabeled bar charts to the left represent the partitioning of species within an individual cluster.\relax }}{41}{figure.caption.32}
\contentsline {figure}{\numberline {1.24}{\ignorespaces \textbf {Comparing individual clusters between node2vec for PCA and t-SNE algorithm output.} The bar chart to the right is the cumelative chart which represents the splits in deciding the cluster a species falls into from \autoref {sec:fsclust}. Unlabeled bar charts to the left represent the partitioning of species within an individual cluster.\relax }}{42}{figure.caption.33}
\contentsline {figure}{\numberline {1.25}{\ignorespaces \textbf {Case Study 1: PCA graph-fingerprint} We compare the functional group distribution for individual clusters within the PCA 2D representation of the graph-fingerprint input.\relax }}{43}{figure.caption.34}
\contentsline {figure}{\numberline {1.26}{\ignorespaces \textbf {Case Study 1: AE SMILES} We compare the functional group distribution for individual clusters within the AE 2D representation of the SMILES input.\relax }}{44}{figure.caption.35}
\contentsline {figure}{\numberline {1.27}{\ignorespaces \textbf {Case Study 1: t-SNE MQN.} We compare the functional group distribution for individual clusters within the t-SNE 2D representation of the Mollecular Quantum Number fingerprint.\relax }}{45}{figure.caption.36}
\gdef \the@ipfilectr {}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {B.1}{\ignorespaces Binary Step activation function.\relax }}{57}{figure.caption.38}
\contentsline {figure}{\numberline {B.2}{\ignorespaces Linear activation function.\relax }}{58}{figure.caption.39}
\contentsline {figure}{\numberline {B.3}{\ignorespaces Sigmoid activation function.\relax }}{58}{figure.caption.40}
\contentsline {figure}{\numberline {B.4}{\ignorespaces Tanh activation function.\relax }}{59}{figure.caption.41}
\contentsline {figure}{\numberline {B.5}{\ignorespaces ReLU activation function.\relax }}{59}{figure.caption.42}
\contentsline {figure}{\numberline {B.6}{\ignorespaces Swish activation function.\relax }}{60}{figure.caption.43}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {C.1}{\ignorespaces \textbf {The labelled co-author network -} as referenced in \autoref {ch3} \relax }}{62}{figure.caption.44}
\addvspace {10\p@ }
\addvspace {10\p@ }
