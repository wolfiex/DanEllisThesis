\section{Conclusions}


The topic of changing climate has been a prominent talking point in the last decade \citep{IPCC2013Science}. Anthropogenic activities starting with the industrial revoluion have served to increase the amount of heat retained by the earth (radiative forcing). Similarly the use of CFCs have damaged the protective layer of ozone in the atmosphere \citep{o3damage}, and dangerous levels of air quality have produced an increase in respiotory distress of living organisms. Although we have guidelines and policies to determine the accepted levels of polutants, it is not uncommon for these to be unregulated or broken - for example it was shown that >95\% of the EU urban population were exposed to concentrations higher than the WHO regulation in \citep{eea}.

To prevent further irreversible harm, both physical and environmental, we must mitigate and attempt to reverse some of the damage already caused. The problem is however, that this is not as small of a feat in itself. Taking the production of ground-level ozone as an example, the complex interplay between emissions and production within the earth system can result in two senarios where the same concentration of a chemical species can result in the production or the loss of ozone based on the chemical regeme it is in. For example \cite{reviewo3} discusses the role of urban vegitation, and how trees can both reduce \citep{losso3} and greatly contrubute to the formation \citep{isopmcm} of ozone - all while the shading provided from tree canopies could also influence the amount of radiation and its production \citep{shady}. As we try to better represent the  processes that gouvern the physical world, the larger and more complex our models become. This means an important balance must be struck, whereupon the `selected'\footnote{In atmospheric chemistry and climate change it is common to compare the results of several different models and mechanisms when studing something new. This style of `ensemble' is provides a good way to check that things are working whilst eliminating some of the errors presented by individual simulations.} tool must be both robust, accurate within reason and computationally efficient.

With the development of construction protocols, the automatic construction for very large, comprehensive, chemical mechanims is now a possiblility, \citep{gecko}. However this presents new problems with trying to simplify it for computation (reduction) and understanding both issues which this issue tried to address.

\subsection{Results}

To solve the problem on interprating these large complex systems we look to how humanity has evolved its ability to store and understand information. Primarily, mankind is a social species. An increase in our neocortex size allows for the effective communications between large numbers of people. Larger numbers of people however present other logistical problems, problems we obercame through external storage (pictograms) and the development of a hive mind.

This sort of information `sharing' proved pivotal for the creation of technology and scientific advancement. Within the thesis explore this info-visual representation of numerical data to provide a data interface attuned to the user. It is found that sorytelling and narrative is a useful method to guide the user through a visualisation. Use of metaphors can instil familiarity and allow a more personal and independant interpretation of a figure, all whilst interactivity and encodings enables them to select filter and extract the information they are interested in. 




This thesis has looked at using new data processing techniques to analyse and visualise the complexity of the chemistry within the atmosphere. It is found that when addressing complex tasks within science, relying on narrative within the visual context allows for the greatest transfer of information to a reader.


Since a mechanism can be thought of a relational representation between reactants and products, a sociograph structure of nodes and links between them proves an effective means of visually communicating patterns in a way that is intuitive.

The force-directed graph structure (a subset of the sociograph class) was found to reduce many of the problems encountered within the conventional (manual) drawing of the chemistry within a mechanism. The push-pull physics nature of these makes them easy to explain while allowing for additional information (such as the rate of reaction) to be embedded within the network shape. This allows for the automatic generation and visual comparison of graphs for different sizes of mechanism, or different points within a chemical simulation.

In addition to the visualisation of a network, it is possible to undergo a level of mathematical analysis using the graph structure. Here we can convert the relationships between nodes for a point in time (as given by the Jacobian matrix) and apply several centrality metrics. These provide information about the number of links of a node,  its role within the network and where the `flow' of information is going. It was established that although node importance as provided by the centrality metrics was useful, it did not provide any additional information to current (and tested) techniques.

In addition to node importance, it also allows us to leverage the graph network and acquire information about its structure. Global graph metrics describe the Master Chemical Mechanism as a sparse graph with the small world (highly connected clusters) and hierarchical (structured) features - properties which are common in real-world graphs. In addition to classifying the type of network, we were also able to use the infomap graph-clustering algorithm to locate the groups within this network. It was found that this can identify modular groups of species that contain many reactions between each other for the CRI mechanism (a reduced version of the MCM). When applying this to the results of several randomly initiated simulations only a couple of small size groups were found, not making it efficient for mechanism reduction through lumping, however, more work needs to be done before this is dismissed as technique.

Finally, in preparation for future research, the use of different species structure representations was run through a number of dimensionality reduction algorithms. Here the different inputs were reduced to two dimensions and plotted in a $x-y$ scatterplot. Analysis of these scatterplots showed that the t-SNE algorithm provided the best spacing between clusters. Additionally, it is found that the type of input can influence the features that are obtained as part of the dimensionality reduction process. It is suggested that if using a neural network, the molecular quantum number or tokenised SMILES input are likely to produce the best results.

\subsection{Conclusions}


\section{Future Work}\label{sec:futurework}
With the newly emerging age of `big data', the fields of data analysis and graph theory are ever-improving. An example of this is the development and use of graph convolutional neural networks in 2016. Here a neural network receives not only information about the structure of an item, but also the relationships it has with everything else. Theoretically, this framework may allow the artificial network to `learn' the relationships and protocols of a chemical mechanism and generate the correct chemical pathways based on the structure of a new (and unseen species).

\newpage

\section*{Reproducability}
\addcontentsline{toc}{section}{\protect\numberline{}Reproducability}%

The code used within this thesis is provided `as is' within the relevant repositories. There will be an attempt to make it more presentable and fully documented within the near future, but this has not yet happened. For many of the tasks it is possible to download a clean repository and implement any relevant changes yourself.

\subsection*{The Box Model}

Most of the work in this thesis relies on the use of the DSMACC Box model \citep{dsmacc}. In order to reproduce it the specific code I have used can be found in \citep{dsmaccgit}, however any box model which allows you to extract both the fluxes and Jacobian matrix may be used.

\subsection*{Photolysis Calculations}

Photolysis rates are calculated with version 5.2 of the Tropospheric and Ultraviolet and Visible codebase. Photolysis rates are calculated once at the start of each box model run and then interpolated with the use of cubic splines to provide the values required throughout the day. This can be located at \citep{tuv}, Photolysis rates within the J array correspond to the lines outlined in \verb|./INPUTS/MCMTUV| and are hard wired within the \verb|./MCMvXX.inc| include files.

\subsection*{The Master Chemical Mechanism}
For the work, we have made use of various versions of the master chemical mechanism \citep{mcm}. Different versions of this and its reduced component (CRI) can be obtained from the MCM website: \url{mcm.york.ac.uk}. Alternatively the KPP presentation of all the mechanisms I have used are located within the \verb|./mechanisms| folder in the DSMACC repository.


\subsection*{Kinetic Pre-Processor}
To transpose the chemical mechanism into a usable format, the Kinetic Pre-Processor rewrites the human readable first order ordinary differential equations into FORTRAN95 code. The version of this originates from FlexChem  - the KPP rewrite used in GEOSChem (KPP 2.3.01). This is located at \url{https://github.com/wolfiex/kpp_2.3.01_gc/}

\subsection*{ML libraries}
Simple processing tasks as clustering , PCA and t-SNE generally make use of the Scikit-Learn package \citep{sklearn}.
Graph Layouts such at TSNET and Mercator can be found in \url{https://github.com/wolfiex/tsNET} and \url{https://github.com/networkgeometry/mercator}.

The AutoEncoder  code can be found within the DSMACC repository at \url{https://github.com/wolfiex/DSMACC-testing/blob/master/dsmacc/examples/rate_ae.py} and the Graph AutoEncoder at \url{https://github.com/tkipf/gae}.


Although not documented, the aim of this thesis was to work up to the use of a graph convolutional network such as the one in \url{https://github.com/wolfiex/gcn}.

\subsection*{Chemial representation and Molecular Keys}
Chemical species representation for SMILES and INCHI strings are taken directly from the MCM. Additional conversions into MACCS and MQN keys make use of the RDKIT  python package: \citep{rdkit}.

\subsection*{Observation and model run reproducibility}
To reproduce the results made from field campaigns it is possible to extract the data directly from the Centre for Environmental Data Analysis. The four field campaigns used are provided below.

\begin{itemize}
 \item{\url{https://catalogue.ceda.ac.uk/uuid/648246d2bdc7460b8159a8f9daee7844}}
 \item {\url{https://catalogue.ceda.ac.uk/uuid/81892deb2dd5e7f0d26b9c587af45f3d}}
 \item{\url{https://catalogue.ceda.ac.uk/uuid/a457d9715f3c4bc295ef975932e491d9}}
 \item {\url{https://catalogue.ceda.ac.uk/uuid/cee49a1f044b79d5413b7a0282467508}}
\end{itemize}

Once downloaded, these are wrangled into the initial conditions CSV format for the use in model runs - some of which are spun up to steady state based on the users preference and aim of the study.

Non-observational runs are initated through the use of a Latin hypercube format to provide a random assortment of initial concentrations within a pre-defined limit. An example of the intial conditions output for one run of these can be found in \url{https://github.com/wolfiex/DSMACC-testing/blob/master/InitCons/lhs_spinup.csv}.

\bibliographystyle{apalike}
\bibliography{bibtex}
