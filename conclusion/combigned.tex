\section{Conclusions}

Humans are social creatures. Our increased neocortex size allows us to interact and communicate effectively between large amounts of people. It is due to this that we created methods for storing information external to our brains and developed a hive mind. Information transfer was pivotal to our development of technology and scientific advancement, however, doing so comes at a cost - and increase of radiative forcing and climate change due to anthropogenic emissions (a problem we are now trying to reduce through the use of scientific understanding and policy).

Since it is not possible to run experiments on the entire planetary system, we are forced to rely on numerical models. Within these reactions of the atmosphere are represented in the form of a mechanism. This thesis has looked at using new data processing techniques to analyse and visualise the complexity of the chemistry within the atmosphere. It is found that when addressing complex tasks within science, relying on narrative within the visual context allows for the greatest transfer of information to a reader.
Since a mechanism can be thought of a relational representation between reactants and products, a sociograph structure of nodes and links between them proves an effective means of visually communicating patterns in a way that is intuitive.

The force-directed graph structure (a subset of the sociograph class) was found to reduce many of the problems encountered within the conventional (manual) drawing of the chemistry within a mechanism. The push-pull physics nature of these makes them easy to explain while allowing for additional information (such as the rate of reaction) to be embedded within the network shape. This allows for the automatic generation and visual comparison of graphs for different sizes of mechanism, or different points within a chemical simulation.

In addition to the visualisation of a network, it is possible to undergo a level of mathematical analysis using the graph structure. Here we can convert the relationships between nodes for a point in time (as given by the Jacobian matrix) and apply several centrality metrics. These provide information about the number of links of a node,  its role within the network and where the `flow' of information is going. It was established that although node importance as provided by the centrality metrics was useful, it did not provide any additional information to current (and tested) techniques.

In addition to node importance, it also allows us to leverage the graph network and acquire information about its structure. Global graph metrics describe the Master Chemical Mechanism as a sparse graph with the small world (highly connected clusters) and hierarchical (structured) features - properties which are common in real-world graphs. In addition to classifying the type of network, we were also able to use the infomap graph-clustering algorithm to locate the groups within this network. It was found that this can identify modular groups of species that contain many reactions between each other for the CRI mechanism (a reduced version of the MCM). When applying this to the results of several randomly initiated simulations only a couple of small size groups were found, not making it efficient for mechanism reduction through lumping, however, more work needs to be done before this is dismissed as technique.

Finally, in preparation for future research, the use of different species structure representations was run through a number of dimensionality reduction algorithms. Here the different inputs were reduced to two dimensions and plotted in a $x-y$ scatterplot. Analysis of these scatterplots showed that the t-SNE algorithm provided the best spacing between clusters. Additionally, it is found that the type of input can influence the features that are obtained as part of the dimensionality reduction process. It is suggested that if using a neural network, the molecular quantum number or tokenised SMILES input are likely to produce the best results.

\section{Future Work}\label{sec:futurework}
With the newly emerging age of `big data', the fields of data analysis and graph theory are ever-improving. An example of this is the development and use of graph convolutional neural networks in 2016. Here a neural network receives not only information about the structure of an item, but also the relationships it has with everything else. Theoretically, this framework may allow the artificial network to `learn' the relationships and protocols of a chemical mechanism and generate the correct chemical pathways based on the structure of a new (and unseen species).

\newpage

\section*{Reproducability}
\addcontentsline{toc}{section}{\protect\numberline{}Reproducability}%

The code used within this thesis is provided `as is' within the relevant repositories. There will be an attempt to make it more presentable and fully documented within the near future, but this has not yet happened. For many of the tasks it is possible to download a clean repository and implement any relevant changes yourself.

\subsection*{The Box Model}

Most of the work in this thesis relies on the use of the DSMACC Box model \citep{dsmacc}. In order to reproduce it the specific code I have used can be found in \citep{dsmaccgit}, however any box model which allows you to extract both the fluxes and Jacobian matrix may be used.

\subsection*{Photolysis Calculations}

Photolysis rates are calculated with version 5.2 of the Tropospheric and Ultraviolet and Visible codebase. Photolysis rates are calculated once at the start of each box model run and then interpolated with the use of cubic splines to provide the values required throughout the day. This can be located at \citep{tuv}, Photolysis rates within the J array correspond to the lines outlined in \verb|./INPUTS/MCMTUV| and are hard wired within the \verb|./MCMvXX.inc| include files.

\subsection*{The Master Chemical Mechanism}
For the work, we have made use of various versions of the master chemical mechanism \citep{mcm}. Different versions of this and its reduced component (CRI) can be obtained from the MCM website: \url{mcm.york.ac.uk}. Alternatively the KPP presentation of all the mechanisms I have used are located within the \verb|./mechanisms| folder in the DSMACC repository.


\subsection*{Kinetic Pre-Processor}
To transpose the chemical mechanism into a usable format, the Kinetic Pre-Processor rewrites the human readable first order ordinary differential equations into FORTRAN95 code. The version of this originates from FlexChem  - the KPP rewrite used in GEOSChem (KPP 2.3.01). This is located at \url{https://github.com/wolfiex/kpp_2.3.01_gc/}

\subsection*{ML libraries}
Simple processing tasks as clustering , PCA and t-SNE generally make use of the Scikit-Learn package \citep{sklearn}.
Graph Layouts such at TSNET and Mercator can be found in \url{https://github.com/wolfiex/tsNET} and \url{https://github.com/networkgeometry/mercator}.

The AutoEncoder  code can be found within the DSMACC repository at \url{https://github.com/wolfiex/DSMACC-testing/blob/master/dsmacc/examples/rate_ae.py} and the Graph AutoEncoder at \url{https://github.com/tkipf/gae}.


Although not documented, the aim of this thesis was to work up to the use of a graph convolutional network such as the one in \url{https://github.com/wolfiex/gcn}.

\subsection*{Chemial representation and Molecular Keys}
Chemical species representation for SMILES and INCHI strings are taken directly from the MCM. Additional conversions into MACCS and MQN keys make use of the RDKIT  python package: \citep{rdkit}.

\subsection*{Observation and model run reproducibility}
To reproduce the results made from field campaigns it is possible to extract the data directly from the Centre for Environmental Data Analysis. The four field campaigns used are provided below.

\begin{itemize}
 \item{\url{https://catalogue.ceda.ac.uk/uuid/648246d2bdc7460b8159a8f9daee7844}}
 \item {\url{https://catalogue.ceda.ac.uk/uuid/81892deb2dd5e7f0d26b9c587af45f3d}}
 \item{\url{https://catalogue.ceda.ac.uk/uuid/a457d9715f3c4bc295ef975932e491d9}}
 \item {\url{https://catalogue.ceda.ac.uk/uuid/cee49a1f044b79d5413b7a0282467508}}
\end{itemize}

Once downloaded, these are wrangled into the initial conditions CSV format for the use in model runs - some of which are spun up to steady state based on the users preference and aim of the study.

Non-observational runs are initated through the use of a Latin hypercube format to provide a random assortment of initial concentrations within a pre-defined limit. An example of the intial conditions output for one run of these can be found in \url{https://github.com/wolfiex/DSMACC-testing/blob/master/InitCons/lhs_spinup.csv}.

\bibliographystyle{apalike}
\bibliography{bibtex}
