
\section{Introduction}

\subsection{Historical Significance}
The established process of trial and error has always underpinned our survival \citep{TrialandError}. Babies are born to rely on a set of sensory reflexes and a framework for physical reasoning \citep{pr}, and with these, we develop methods to navigate the influence of change within a physical, and auditory space \citep{objects}. This method of decision making is reflected in our adult lives with ideas and actions being limited in choice by our intuition and experience \citep{descartes}. In science, we apply a methodological framework consisting of a continuous assessment of scepticism, educated guessing (hypothesising) and rigorous practical testing. Specialists accrue years of practical and theoretical knowledge within a narrow field and can identify areas of potential gain and futility. Nevertheless, even with all prior experience, the discovery of new and untested techniques involve the tortuous traipsing through a sea of uncertainty.


 Such methods sometimes prove fruitful, through accidental discoveries of items such as polyetheylene, penicillin, x-rays, nylon, teflon, velcro etc.   \citep{accidental,serendipity}; finding novel applications for existing methods such as optical tweezers for chemistry or the abstract field of maths utilised by Einstein, but more often than not end in the constant evolution of a pre-existing project with no apparent result.

\subsection{Theory And Simulation In Science}

Until recently much of the experimentation possible was limited by resources, levels of knowledge and available technology. With the increase of computation power, we have been able to not only increase our understanding but also run theoretical simulations to guide exploratory efforts with an impact on real-world applications \citep{dft,lion,theoreticalbio,drug}. However, as our ability to record and produce data increases, the need for the scientific method diminishes \citep{wired}. Here the application of `big data' tools and algorithms can provide insights and correlations much more compelling than the predictive capabilities of constantly changing models - ``Since all models are wrong the scientist cannot obtain
a "correct" one by excessive elaboration'' - \cite{allmodels}. As our level of attainable technology increases, so does the complexity of the data collected. New datasets tend to be very large, complex and highly multivariate. Although this dramatically improves the quality of science, the difficulty lies in trying to represent it in such a way that we may efficiently access the reliability of the results. Since simple bar and line graphs are no longer applicable, one solution falls within a class of unsupervised machine learning techniques called dimensionality reduction (DR).


\subsection{Chapter Aims}
In \autoref{ch1}, we looked at visual representation as a way of better understanding of complex systems. \autoref{ch2} showed that the chemical properties could be inferred (visually) from the node-link graph structure of a mechanism. Similarly, \autoref{ch3} and \autoref{ch4} located the presence of important species and clusters of similar properties by applying mathematical algorithms to the graph network. As opposed to attempting to visualise complex data, this chapter looks at learning the structure of a chemical species and simplifying it into two dimensions. Here it is possible to extract key features of like-groups through the use of vector clustering, which unlike the graph clustering in \autoref{ch4} works by determining the density between points on a plane.

The chapter begins with the introduction of the chemical system, and the various methods for representing species structure within it (\autoref{sec:drinput}). Next, we define the dimensionality reduction methods, which are to be used to simplify the inputs above (\autoref{sec:dr}). This is followed by a brief overview of the visualisation methodology (\autoref{sec:drvis}). Finally, all three sections are combined to produce a set of result and conclusions about the use of DR to identify species structure. The aim of this chapter is to access the efficiency of a machine learnt (dimensionality reduced) models in simplifying the chemical structure, and decide upon the best input for future deep learning tasks (e.g. for mechanism construction and emulation).



\section{Species Of The MCM And Ways To Represent Them.}\label{sec:drinput}
The master chemical mechanism (as defined in all previous chapters), represents our foremost knowledge of gas-phase chemistry within the troposphere.  \autoref{ch2} shows that information about a species structure is encoded within its reactions, much of which can be attributed to the well-defined construction protocols.

This section explores the different methods of representing a species structure, intending to provide a machine built algorithm with the highest amount of information about each species and its functionality. A range of input types will be evaluated against several dimensionality reduction algorithms to isolate which chemical properties are most extractable.

\subsection{Input Generation}
The MCM database provides species information in the form of a species `SMILES' (\autoref{sec:SMILES}) and the IUPAC InChi string \citep{inchi}. Within this chapter, we use only the SMILES string, which is either manually processed using regular expressions or with the aid of pythons RDKIT package \citep{rdkit}. There are seven different methods for representing the chemistry; these are outlined below.


\subsection{Manual Categorisation}
Reactions within the MCM are determined by a set of rules (\autoref{fig:protocol}). These mimic the process a chemist may use to build a species degradation mechanism and often rely on understanding the bond availability and functionalisation of a species. Since the present functional groups are the benchmark of whether a DR algorithm has successfully separated species structure, it makes sense to run a unit test using the known functional groups of a species as the input.

To generate the functional groups the regular expressions in \autoref{tab:fngroups} are used\footnote{To see the structure of each functional group type, go to \autoref{appendix:fngroups}.} on the SMILES strings (described in \autoref{sec:SMILES}) for each species. In extracting the functional groups, we can plot the likeliness a species with a certain group is likely to have another using a chord diagram - \autoref{fig:covermcm}. Since most species contain a multitude of functional groups, the separation of these into `tidy' clustered groups seems unlikely.


%



\begin{table}[H]
    \centering
    \begin{tabular}{c|p{5in}}


PAN & \verb! C\\(=O\\)OON\\(=O\\)=O$|^\\[O-{0,1}\\]\\[N\\+{0,1}\\]\\(=O\\)OOC|!\\&\verb! O=N\\(=O\\)OOC\\(=O\\)|C\\(=O\\)OO\\[N\\+{0,1}\\]\\(=O\\)\\[O-{0,1}\\]!\\&\\

Carb. Acid & \verb! [^O](C\\(=O\\)O$|^OC\\(=O\\))!\\&\\

Ester & \verb! [\^O](C\(=O\)O\b|OC\(=O\))C!\\&\\

Ether & \verb! (\([\^O=]+\))*C(\([\^O=]+\))*O(\([\^O=]+\))*C(\([\^O=]+\))*!\\&\\

Per. Acid & \verb! c\\(=O\\)OO$|^OO\\(=O\\)C!\\&\\

Nitrate & \verb! O(NO2\b|NOO\b|N\(=O\)=O|\[N\+\](?:\[O-\\]|\(=O\)){2})!\\&\\

Aldehyde & \verb! C=O$|^O=C!\\&\\

Ketone & \verb! C\(=O\)C!\\&\\

Alcohol & \verb-CO\\b|(?=^\\b)(?!^\\[)CO.|(?=^\\b)(?!^\\[)OC.|\\(O\\)|C\\)O(\\b|[^O]-\\&\\

Criegee & \verb! \[O-\]\[O\+\]!\\&\\

Alkoxy rad & \verb!\[[\/]{0,1}CH{0,1}\]|\b[\^O]\[O\.{0,1}\]!\\&\\

Peroxyacyl rad & \verb! \\ w\(=O\)O\[O\.{0,1}\]!\\&\\

    \end{tabular}

    \caption{ A set of regular expressions that are used to determine the number of occurrences of a functional group within a SMILES string. These were written to scan the SMILES string a match specific patterns corresponding to each functional group. A similar process is used within \citep{rdkitcode} to construct MACCS keys (discussed later).}
    \label{tab:fngroups}
\end{table}\footnote{Check the correct table has been used.}



\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{4fig/coverfig.jpg}
    \caption{\textbf{The multifunctionality of the MCM.} A chord diagram showing the functionalisation of all species within the MCM v3.3.1. Arc sizes represent what percentage of all functional groups in the MCM mechanism a group contains. Translucent areas of no outwards links represent species with multiples of a certain functional group, of which Alcohols and Ketones have the most.
    Source: \citep{cover} }
    \label{fig:covermcm}
\end{figure}



\subsection{Tokenization}
As computer algorithms are unable to understand words or their meaning, we have first to categorise the data into groups. Tokenisation is the conversion of a string into characters and representing them with a numerical equivalent. In doing so, a string of characters can be converted into a numerical vector, allowing for its representation in a latent vector space.
Within our input selection, we have two sets of inputs we can convert. These are the species names, and their SMILES string representation.



\subsubsection{Species Names}
In \autoref{ch4} it was shown that the dedicated species names for species in the CRI mechanism were often representative of their structural properties. This also applies for the MCM, where an intuitive naming convention following the FACSIMILE format is used. This is often derived as part of the construction protocol, where a species names reflect its own, or its precursor's structure (which it will have at least in-part inherited).

Although this is not the most robust method of defining the structure, it allows for a straightforward test of the algorithms, for which the user can quickly compare the human-readable output.


\subsubsection{SMILES Strings}\label{sec:SMILES}


 SMILES (`Simplified Molecular-Input Line-Entry System') provide a human-readable representation of the molecular structure,

 \citep{smiles}. They offer a linear human-readable description of the chemical composition within a molecule - making it easy to visually check the construction of a species without any additional work. Besides, their role in generating the molecular fingerprints in \autoref{sec:fingerprints}, SMILES strings provide a useful tool for quickly comparing species structure.

\paragraph*{Construction Methodology of SMILES strings}
The construction of a SMILES string happens in three parts:

\begin{enumerate}
    \item The SMILES string is built by creating the longest possible chain to form a molecule backbone.
    \autoref{fig:st2}

    \item This may within itself contain aromatic rings denoted by the lowercase carbons and a number corresponding to the location of each break cycle. \autoref{fig:st3}

    \item Finally all the functional groups and branches attached to the main backbone are added. These are nested within the parenthesis to show that they are not part of the skeletal backbone. \autoref{fig:st4}
\end{enumerate}



\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.495\textwidth}
         \centering
         \includegraphics[width=\textwidth]{4fig/sm4.png}
         \caption{Structure of Melatonin}
         \label{fig:st1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.495\textwidth}
         \centering
         \includegraphics[width=\textwidth]{4fig/sm1.png}
         \caption{Step 1 : Building the C chain backbone.}
         \label{fig:st2}
     \end{subfigure}\\

     \begin{subfigure}[b]{0.495\textwidth}
         \centering
         \includegraphics[width=\textwidth]{4fig/sm3.png}
         \caption{Step 2 : Aromatic Rings}
         \label{fig:st3}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.495\textwidth}
        \centering
            \includegraphics[width=\textwidth]{4fig/sm2.png}
            \caption{Step 3 : Functional Groups }
            \label{fig:st4}
        \end{subfigure}

        \caption{ \textbf{Construction process of a SMILES string.} The example compound is Melatonin. Although this does not exist within the atmosphere, it provides a clear example of the SMILES string methodology. \autoref{fig:st1} is made using SMILES drawer: \citep{SMILESdrawer} }
        \label{fig:SMILES}
\end{figure}


\subsection{Graph Inspired}

\autoref{ch2} - \ref{ch4} have shown the role of graphs in revealing network properties and structure. Graphs in themselves can simplify relational data into two/three dimensions for visualisation and algorithmic clustering. Continuing this trend, we can represent a species structure in the form of a graph (\autoref{sec:specgraph}), as well as converting the structure of a mechanism for dimensionality reduction (\autoref{sec:n2vec})


\subsubsection{The Species Graph (Fingerprint)}\label{sec:specgraph}

The structure of a species has long represented using a graph-like layout, \autoref{ch2}. It, therefore, follows that other methods for representing the graph structure would also apply. One such way is the use of an adjacency (or relational) matrix to describe the relationships between atoms and bonds in a species. Such a methodology is already used in the construction of bond and z-matrixes \citep{mcmgen,zmatrix}.

The construction of a structure matrix/graph begins with a chemical species. Here the relationships between atoms (\autoref{fig:graphmol}) is converted into an adjacency matrix (\autoref{fig:adjmol}). However, since species have different numbers of each atom, a template allowing us to compare different graphs is required. To do this a maximum occurrence table (\autoref{table:my_maxoccur}) is created. Here, for example, $\beta$-caryophyllene (BCARY) \ch{C15H24}, contains the most carbon atoms of any species within the MCM. This universal matrix is now able to contain any possible combination of atoms in a species.

As machine learning algorithms only use vectors as an input, it is possible to decompose the $37^2$ element adjacency matrix into rows, which can then be joined together, Using this method we create a one-dimensional array (vector) of 259 elements (518 bytes) to represent our species.


\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.325\textwidth}
         \centering
         \begin{tabular}{c|c}
         \textbf{Atom} & \textbf{Max}\\
         \hline\hline
         &\\
             C & 15 \\
             Cl & 4 \\
             O & 12\\
             N&3\\
             S&1\\
             BR&2\\
         \end{tabular}
          \caption{}
         \label{table:my_maxoccur}
     \end{subfigure}
     \begin{subfigure}[b]{0.325\textwidth}
         \centering
         \includegraphics[width=\textwidth,height=.8\textwidth]{4fig/INB1NBCO3.pdf}
         \caption{}
         \label{fig:graphmol}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.325\textwidth}
         \centering
         \includegraphics[width=\textwidth,height=.8\textwidth]{4fig/INB1NBCO3_adj.png}
          \caption{}
         \label{fig:adjmol}
     \end{subfigure}

        \caption{ \textbf{Constructing a graph from species structure.}
        (a) shows the maximum number of times an atom occurs for any single species in the MCM. (b) depicts the graph-like chemical structure of {INB1NBCO3}(a product from isoprene). This is a highly processed species stemming from Isoprene, and this makes for a good example of the bond matrix. Finally, a matrix representing the bonds in \ce{INB1NBCO3} is created from the maximum possible occurrence matrix from (a). For simplicity, empty row/column pairs have been removed to produce (c). This matrix will always be symmetrical as the bonds do not have a direction.}
        \label{fig:bondmat}
\end{figure}


\subsubsection{Node Embeddings (Node2Vec)}\label{sec:n2vec}
\autoref{ch2} and \autoref{ch3} showed that the underlying structure of a chemistry mechanism graph contains information about the species and reactions within it.  Here as a species is oxidised the O-C ratio increases. Long-chain VOCs are likely to fragment into two radicals, producing smaller more oxidised species. Eventually, this process leads to the production of carbon dioxide and water. \autoref{fig:vk} shows a subset of the MCM representing the chemistry in Beijing. Node colour and size show the increase of oxidation as species head towards CO at the centre) - lighter colour and larger node.

This type of structural information can be extracted through the use of a natural language processing package capable of transforming a graph into a vector - node2vec \citep{node2vec}. Since this may also be used for dimensionality reduction, it is described within the next section (\autoref{sec:n2v}).


\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{4fig/graph/oxidised_ratio.png}
  \caption{\textbf{A graph of an MCM subset representing the chemistry within Beijing.} Here colours show the increase of \ce{O-C} ratio as species are oxidised (lighter). All emitted species ultimately tend towards carbon monoxide, which is at the centre of the graph. Node clusters symbolise groups of species which react more between themselves and less with others (This graph only represents the mechanism structure).}
  \label{fig:vk}
\end{figure}





\subsection{Molecular Fingerprints}\label{sec:fingerprints}

In the field of chemical informatics, molecular fingerprints (or structural keys) are used to encode and query structural properties of species. Their binary representation makes them suitable for dimensionality reduction and the exploration of chemical space (a type of property space constructed using pre-determined features and boundary conditions).

Here species properties are often split into structural and psyico-chemical groups - which has used such as the discovery of natural analogues (which circumvent problems such as intolerances in medicine\footnote{Certain molecules can cause allergies in people, but whist their natural analogues do not. } \citep{analog}). Although there exist many different types of molecular fingerprints, the two main ones that will be explored are molecular quantum numbers (MQN) and the molecular access system (MACCS).

\subsubsection{Molecular Quantum Numbers (MQN)}
In chemistry the shape, phase and electron occupancy of an atom may be described through the use of four quantum numbers: the $n$ principle quantum number, $I$ angular momentum quantum number, $M_i$ magnetic quantum number and $M_s$ spin quantum number. The rationalisation of elements based on their structure, and by consequence reactivity, has led to the most iconic tool of the modern-day chemist - the periodic table, where increasing atomic numbers follow the principal quantum number \citep{periodic}. In representing a molecule as a set of 42 quantum numbers, MQN fingerprints produce a multi-dimensional mapping of an atom, bond, polarity and topology count \citep{MQN}.

\subsubsection{Molecular Access System (MACCS)}
MACCS keys are a 164\footnote{They are 166-bit keys, although there is no real agreement to what the 44th keys' purpose is, and therefore it is often omitted. Within RDKIT this is denoted by a $?$ \citep{rdkitcode}.} bit structural keys formulated through answering a series of structure-related questions. Developed by MDL Information Systems \citep{maccs}, their main purpose lies in being a SMILES Arbitrary Target Specification (SMARTS) system for substructure searching. However, their distinct structure key format makes them highly suitable for similarity detection. In many cases, the optimised version of MACCS keys is cited (\citep{optimised}), although most use cases exploit a variation of the undocumented 166bit keys. We use the implementation presented by \citep{rdkit,rdkitcode} for all molecular fingerprints in our work.


\section{ Dimensionality Reduction Methods}\label{sec:dr}
In the last section, we described several methods in which the chemical structure of a species could be encoded for direct comparison. However, since each input consists of a multitude of elements, it is still not a simple task to determine the differences and similarity between all species in mechanisms. Dimensionality reduction is the process of reducing the number of random variables and only presented a set of principal values, by mapping a high-dimensional space into a low-dimensional one \citep{drrandom}. This allows us to flatten a multivariate input into the two dimensions required for a simple scatter plot.

In this section, we begin by explaining the data preparation required for dimensionality reduction (\autoref{sec:prep}) before describing the different possible methods of reducing the dimensions of a dataset through Principle Component Analysis, Auto Encoders and t-Distributed Stochastic Neighbor Embedding.

\subsection{Preperation Of The Data}\label{sec:prep}
Real-world data is rarely preformatted in such a way that it can be used directly within a computational model. Often values need to be cleaned and corrected to be fit for purpose. In the interest of completeness, the two main methods of data adjustment for machine learning are outlined below. These are (i) normalisation and (ii) standardisation.


\subsubsection*{(I) Normalisation}
In the data is without (dimensionless) or of a single unit, it is possible to rescale the data between a range - most commonly {0,1}. In doing so, it is possible to interpret the importance of value in contrast to the largest recorded value. This gives us a percentage scale spanning the range of the data. Such a range is useful in the definition of colourmaps and describing the importance of value relative to the dataset.
To rescale a dataset, we shift the minimum value to zero, then divide by the new maximum of the dataset (Note this is equivalent to the range of the unshifted dataset.)

\begin{equation}
    n(x_i) = \frac{x_i - \min_x }{\max_x - \min_x}
    \label{eqn:n}
\end{equation}



\subsubsection*{(Ii) Standardisation}
If the components we wish to compare are of different units or are expressed with a different scale, normalising them can not produce meaningful data. Instead, it is possible to standardise the data by looking at each points deviation from the mean. Here the variation of the mean for a dataset is divided by the standard deviation to produce a value between \{-1,1\}, \autoref{eqn:z}. In statistics this is known as the `z-score'\footnote{Possibly because of the American spelling of standardi\textbf{Z}ation?}

\begin{equation}
    z(x_i) = \frac{x_i - \mu_x}{S}
    \label{eqn:z}
\end{equation}\\


\subsection{Principle Component Analysis (Pca)}
One of the most well-known dimensionality reduction methods is the determination of the principal components through the use of Principal Component Analysis (PCA). PCA increases the readability of a dataset by creating a set of new uncorrelated variables which maximise the variance \citep{pcareview}.


 PCA works on the assumption that components within a dataset are linear combinations of each other. By simplifying these linear combinations, it is possible to identify the elements which explain the most variability in a dataset - these are the principal components.

A more straightforward interpretation of this would be to adjust the direction of each axis of the data, such that its projection has the most prominent variability. In doing so, it is possible to determine which components contribute the most to changes in the dataset \citep{pca,pca2}. An example of this is seen in \autoref{fig:2dpca}, where the second component of the original data can be removed with little effect on the overall result of the data. Such methods have applications in compression and signal filtering \citep{pcacompress,pcanoise}.


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{./4fig/pca2d.png}
    \caption{\textbf{Determining the Principal Compnent of a sample dataset.} It can be seen that in a change in axis to follow the first principal component (right), it is possible to explain most of the variation in the samle dataset (left). Source: \citep{pcaim}}
    \label{fig:2dpca}
\end{figure}


\subsubsection{Mathematical Explanation Of Pca}
\emph{\textbf{Note:} The basic statistics/mathematics required to understand this section is shown in \autoref{apendix:pca}. Please read this if you are not familiar with any of the terms below.
}

The mathematics behind PCA consists of first calculating the covariance matrix - a $n \times n$ matrix outlining how strongly each variable changes with every other. Using this we can calculate both the eigenvalues and eigenvectors of the matrix \footnote{These need to be unit vectors, although most packages already do this out of the box.}. This can be done using a computational package such as numpy or scipy \citep{numpy,scipy}.

We can now sort the eigenvector columns by influence using their eigenvaluesâ€”this way a feature dataset can be produced by removing vectors of low importance. The final feature dataset can now be transposed and multiplied by the transpose of the original dataset. This results in an output dataset containing each principal component of the desired dimension.



\subsection{T-Distributed Stochastic Neighbor Embedding (t-SNE)}\label{sec:overcrowd}

t-SNE is an algorithm designed with visualisation in mind \citep{tsne}. Rather than representing the data through a series of linear transformations, t-SNE uses local relationships to create a low-dimensional mapping, much in the same way as a fully connected force graph, as shown in \autoref{fig:tsneforcegraph}. This allows the ability to capture non-linear structures in the data which cannot be accomplished through linear mapping methods (e.g. PCA).

\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{./4fig/tsneforcegraph.png}
    \caption{\textbf{Representing the t-SNE algorithm as a fully connected force graph.} Here each node is attached to every other node. Nodes with a strong relationship are pulled closer together than those with a weaker one.}
    \label{fig:tsneforcegraph}
\end{figure}



The algorithm itself can be simplified into two parts, (see A and B below), and is described in \autoref{sec:mathtsne}.

\begin{itemize}
  \item [A.] Create a probability distribution which dictates relationships between neighbouring points
  \item [B.] Recreate a lower-dimensional space following the probability distribution established in A.
\end{itemize}

The main reason t-SNE produces good results is that it can handle the \textbf{`crowding problem'} very well. The crowding problem is a product of the `curse of dimensionality' \citep{curse}. In a high dimensional space, the surface of a sphere will grow much quicker than one in a lower dimension space. This means that the higher dimension spaces will have more points at a medium distance from a certain point, \autoref{fig:dimcurse}. When we map our data into a lower dimension, data will try to gather at its medium distance, resulting in a more `squashed', and thus crowded, output.



\begin{figure}[H]
  \centering
\includegraphics[width=.5\textwidth]{4fig/dimcurse.pdf}
\caption{\textbf{An example of how the curse of dimensionality affects the mapping of points a certian distance from eachother.} }\label{fig:dimcurse}
\end{figure}




\subsubsection{Mathematical Explanation Of t-SNE}\label{sec:mathtsne}

%

In the original paper \citep{tsne}, the algorithm is described using the etymologic dissection of its name (described below).

\paragraph{(i) Step 1}
First we begin with Stochastic Neigbour Embedding (SNE) - the distribution across neignbouring datapoints in our high dimension space. This is done by converting the high dimensional Euclidian distances between points into conditional probabilities representing their similarity:

\begin{equation}
p_{ij} = \frac{\exp(-\left \| x_i - x_j \right \|^2 / 2\sigma_i^2)}{\sum_{k \neq l} \exp(- \left \| x_k - x_l \right \|^2 / 2\sigma_i^2)}
\end{equation}

Here $p_{i|j}$ is the conditional probability that $x_i$ may pick $x_j$ as a neigbour. This is proportional to the probability density of a Gaussian $\sigma_i$ centered at $x_i$.

\textbf{Perplexity}\\
Since we want the number of neighbours of each point to be similar in number and prevent a single point from having a disproportionate influence on the entire system we introduce a hyperparameter named \emph{perplexity}. Perplexity works by ensuring that $\sigma_i$ is small for points in densely populated areas and large for spare ones and can be thought of as a scale of the number of neighbours considered for any one point in the system. Generally, values between 5 and 50 are considered to give good results, with larger perplexities taking global features into account, and by consequence smaller ones, local features \citep{tsne}.

\paragraph{(ii) Step 2}
Now a probability distribution describing the relationship between points has been formulated, we wish to express this as a low dimensional mapping of our inputs $X$ in terms of our output dimensions $Y$. Naturally, we would want to make the low dimensional mapping represent a similar (Gaussian) distribution as in Step 1. However, it often causes issues presented by the `overcrowding problem', \autoref{sec:overcrowd}, as the gaussian has a `short tail', and thus nearby points are likely to be pushed together. A solution to this is the student t-distribution which has a longer tail \footnote{The distribution employed is a t-distribution with only one degree of freedom and is identical to the Cauchy distribution}:

\begin{equation}
q_{i|j} =\frac{(1 + \left \| y_i - y_j \right \|^2 )^{-1}}{\sum_{k \neq l} (1 + \left \| y_k - y_l \right \|^2 )^{-1} }
\end{equation}

\emph{\textbf{Note:} The definition and explination of the Student t-distribution is given in \autoref{apendix:tsne}.
}

The optimisation of this equation is achieved through the use of \emph{gradient decent}\footnote{\textbf{Gradient Decent} - an optimisation algorithm used to minimise a function by iteratively moving in the direction of the steepest descent. Gradient descent is used to find local minima and is defined by the negative of the gradient of the system. Its primary usage in machine learning is the updating of parameters (coefficients in linear regression and weight in neural networks).}
 on the Kullback-Leibler divergence \autoref{appendix:kl} between distributions $p$ and $q$. Here the gradient is used to apply an attractive and repulsive force on the items\footnote{A positive gradient signifies attraction, while a negative one corresponds to repulsion.}.




\subsection{Pca Vs t-SNE, A Quick Comparison.}

PCA has been around for much longer than t-SNE, and its uses are well established within the scientific community. In essence, an example of this give by \cite{wyche} where mechanisms can be separated into different pathways (on account of the underlying chemistry) and \cite{kinetics} where sensitivity analysis is used within mechanism reduction. It is fast, simple and easy to use and very intuitive. The PCA algorithm works by creating a lower-dimensional embedding which best preserves the overall variance of the dataset. Clusters created from the algorithm are grouped in ways, such that they retain the highest variance of the data.

The main drawback of PCA is that it is a linear projection. If our data happened to be in a `swiss roll' (spiral) pattern, we would not be able to `unroll' it. The reason for this is that the PCA algorithm works by viewing the data from different perspectives, much like casting a shadow from various directions. With such an example, there is no one way we can do this that unfurls the spiral.

t-SNE, on the other hand, is a relatively new method \citep{tsne}. Its greatest asset is that linear projections do not limit it. Although more computationally intensive for large datasets, t-SNE produces visibly cleaner results. Unlike in PCA, t-SNE cannot be trained on additional data at a later point; however, the output clusters are more visually distinct (they have less of an overlap). Much like in a force graph, the output from t-SNE is scale-invariant. This means that while the location of clusters in a PCA reduced representation has an attributable quality, those produced by t-SNE will not necessarily contain the same information.

A box model run representative of the chemistry within Beijing was used to compare the differences between PCA and t-SNE. The aim is to classify the diurnal profiles of each species concentration (much like the cosine similarity in \autoref{sec:cosine}).
Diurnal profiles were extracted on the third day of a spun up model initialised with initial conditions representative of the chemistry within the Beijing environment (\autoref{tab:icsmetric}).
These were then standardised and converted into temporal vectors for use in the algorithms.

\autoref{fig:threegraphs} shows the output of both dimensionality reduction algorithms on the dataset. Different colours represent the location of clusters of similar diurnal profiles. A higher dispersion between clusters and species overlap is seen within the PCA output, \autoref{fig:pcac}. This makes it harder to distinguish species from each other or other groups around them.
Since the distance between clusters within t-SNE does not hold the same mathematical meaning as PCA, the algorithm can provide a better distribution of points, creating better-defined clusters, \autoref{fig:tsnec}. The concentration profile shapes for each coloured group is shown in \autoref{fig:tco}.


\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.495\textwidth}
         \centering
         \includegraphics[width=\textwidth]{4fig/ppca.png}
         \caption{PCA}
         \label{fig:pcac}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.495\textwidth}
         \centering
         \includegraphics[width=\textwidth]{4fig/ptsne.png}
         \caption{t-SNE}
         \label{fig:tsnec}
     \end{subfigure}
     \hfill \hfill
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=\textwidth]{4fig/ptsneall.png}
         \caption{t-SNE with cluster outlines.}
         \label{fig:tco}
     \end{subfigure}
        \caption{\textbf{Showing the difference between PCA and t-SNE clustering.} These figures show the clustering of a set of standardized species concentration profiles (c) across two styles of dimensionality reduction: PCA (a) and t-SNE (b). }
        \label{fig:threegraphs}
\end{figure}




\subsection{The Auto-Encoder (Ae)}\label{sec:ae}
Auto-encoders are a subclass of neural networks with primary use in compressing data (dimensionality reduction). Rather than predicting a numerical output, AutoEncoders focus on the construction and deconstruction of data through the use of an encoder and decoder pair. The encoder takes an n-dimensional input and applies a compression, reducing it to the number of dimensions in the bottleneck layer. The reduced dataset is then reconstructed within the decoder. Such a process not only allows for an easy understanding of the error of the reduced data but can also be used in the filtration of noisy or pixelated data \citep{aenoise,aeim} and as an input to more complex machine learning models.\


\begin{figure}[H]
\includegraphics[width=\textwidth]{4fig/ae.pdf}
\caption{An example autoencoder structure which reduces a 16 dimentional input to 2. Draw with the aid of \citep{drawae}}
\end{figure}



There are two features of an autoencoder that make it a particularly powerful tool. The first is the ability to sample the latent space using the decoder. The implications of this are that we can establish features that correspond to gaps between our data points - which can have its application if the data used is sparse or incomplete. Next comes the inherent non-linearity of the model. As an autoencoder is just a neural network, the amount of information passed through each link between layers is governed by an activation function. Should this activation function be linear, the reduced dimension will be much akin to a PCA decomposition. Where PCA reduces the dimensions of a dataset by discarding those with little effect on the variance, an autoencoder opts to combine it. Here the entirety of the dataset remains encoded within the links of the AE network. To decide how data flows along the edges of the network, a series of threshold (activation) functions are used for each layer. These are described in \autoref{appendix:activation}.



\subsubsection{Demonstration Of Non-Linear Activation Functions}

To demonstrate the effect of these features, we take a sample isopleth of Methane, NOx and Ozone from 300 box model simulations and reduce it to two dimensions. This is then reconstructed back into three dimensions using the DR algorithms.
\autoref{fig:aeiso} shows the difference between the original dataset (\autoref{fig:iorig}) and that of the PCA (\autoref{fig:ipca}) and AutoEncoder (\autoref{fig:iae}) reconstructions. Here we see a loss in the non-linearity of the original data for the PCA reconstruction. However, the use of a non-linear (tanh) activation function within AutoEncoder produces a result much closer to the original. Use of a linear activation function, however, produces a similar result to the PCA algorithm.

\begin{figure}[H]
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\textwidth]{4fig/original.png}
  \caption{Original}  \label{fig:iorig}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\textwidth]{4fig/rpca.png}
  \caption{PCA}  \label{fig:ipca}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\textwidth]{4fig/rae.png}
  \caption{AE (Tanh)}\label{fig:iae}
\end{subfigure}%
\caption{
\textbf{Comparing the result of the 2D encoding and decoding of an Ozone-NOx-Methane isopleth.} The original data (a) is reduced to two dimensions and then reconstructed back into 3D. This is done with Principal Component Analysis (b) and an AutoEncoder (c). The original isopleth is created using  300 simulations of different
 intial conditions: NOx (variable), Methane (variable) and Ozone (constant). These were designed using a latin hypercube and converted into a surface plot using Delaunay triangulation. }
\label{fig:aeiso}
\end{figure}

\subsection{Node2Vec}\label{sec:n2v}
Finally, Node2Vec is an embedding algorithm designed to generate vector representations of the nodes in a \textit{undirected} and \textit{unweighted} network. Although it can be used to reduce a complex network into a 2D vector (dimensionality reduction), for this work we shall only use it to generate a fingerprint for a species' position within a mechanism network graph - and then apply this as an input to the DR methods above. This method of input creation has been found more computationally efficient, by circumventing the need for expensive composition, in producing better predictions on network-related tasks compared to more classical methods such as PCA \citep{node2vec}.


\begin{figure}[H]
  \centering
\includegraphics[width=\textwidth]{4fig/n2vproc.png}
\caption{The process of converting a graph into a vector using Node2Vec. Source:\citep{n2vimg}}\label{fig:n2vprocess}
\end{figure}


The process of converting the graph structure (\autoref{fig:n2vprocess}) into a numerical vector node embedding starts by taking a series of $2^{nd}$ order random walks. These describe the neighbourhood of a node in the form of a set of random walk paths, much in the same way words are dependant on their neighbours within a sentence- for example in the OH initiated degradation of isoprene in the MCM result in the following path along with the mechanism graph.:


\begin{equation}
ISOPRENE \rightarrow OH \rightarrow TISOPA \rightarrow ISOPBO_2 \rightarrow TISOPA \rightarrow...
\label{eqn:w2varrow}
\end{equation}

This methodology allowed for the use of word2vec algorithm, converting the walk into a vector (\autoref{sec:w2v})



\subsubsection{Sentence Construction By Sampling Of A Network}
The probability and path (as described above) depend both on a set of arguments, and a random seed\footnote{Computers can never generate truly random numbers. If we want reproducibility within models, the random number generator can be initiated with the same seed.} provided to the model.

\begin{figure}[H]
  \centering
\includegraphics[width=0.5\textwidth]{4fig/n2vedge.png}
\caption{Calculation of the random walk path. Source:\citep{node2vec}}\label{fig:n2vedge}
\end{figure}


 \autoref{fig:n2vedge} shows the return and input parameters ($p\ \&\ q$) determine how fast we explore the network and our probability to leave the neighbourhood. In a system, where the previous path is from $t$ to $v$, we may calculate the probability of returning to $t$ as $1/p$, going to a mutual node connected between $t$ and $v$ as 1, and viewing a new node as $1/q$.
If $q>1$ we have a high probability to end up at nodes close to $t$, and with $q<1$ we are likely to explore other nodes. Additionally if we chose $p> \max{q,1}$ we are less likely to return to an already visited node ($p < \min{q,1}$ is likely to generate a backwards step). Since we wish to generate a `local' view, but do not wish to return to $t$ we select  $q \ge 1$ and $p > q$ our parameters as  $p = 2.0,q=1.1$.  In the case of a weighted graph (something that we are \textit{not} exploring within this chapter) the resultant $alpha$ value calculated is further multiplied by the edge weight.

To generate the node2vec embeddings for each species, we use the python2 code provided by the original paper by \cite{node2vec} with a set of 50000 random walks, each of length 9 product/reaction generations. The reasoning behind this is that we have a large graph, with a power-law like structure (where species are often heavily connected, \autoref{ch3}).

\textit{NOTE: This process takes over a week to compute (in serial), and then the binary file containing all walks in character form approaches 10 GB, for the complete MCM. }




\subsubsection{Word2Vec}\label{sec:w2v}
Once we have constructed our random path `sentences' (e.g. \autoref{eqn:w2varrow}), we can make use of Googles word2vec algorithm \citep{w2v}. This is similar to an auto-encoder in many regards; however, the algorithm looks at neighbouring words (or species) in the corpus rather than learning word embeddings using reconstruction. This form of representation has found many uses beyond the realm of natural language processing. Some of these are objects, people, code, tiles,genes and graphs \citep{objects,people,code,tile,gene,graph2vec}.


%
%

\subsection{Summary Of Dimensionality Reduction Methods}
There exist several methods of reducing a complex dataset into a smaller one. PCA is the simplest method to understand but is constrained to linear decompositions. AutoEncoders can have both a linear and non-linear response, based on the activation functions that they use, and t-SNE applies a non-linear grouping which mimics a complete force-directed graph.

Having defined each method, we next explain how they will be evaluated (\autoref{sec:drvis}), before applying them to the MCM in \autoref{eqn:w2varrow}.




\section{Visualisation Of Clustering}\label{sec:drvis}

In assessing the validity of clustered space, we require a level of exploratory data analysis. To reveal features of interest, we plot the reduced 2D dataset and apply interactivity coupled with a selection of visualisation techniques described below. This section outlines the different visualisation methods used.

\subsection{Viewing The 2D Species Embeddings}
Since the different DR algorithms return data on various scales, comparison between the outputs is not straightforward. To overcome this outputs in $x$ and $y$ are normalised (scaled between \{0,1\}), \autoref{sec:prep}, before being plotted as a scatterplot.


\subsection{Exposing Overlapping Data}
If the nodes within a tight-knit cluster overlap, this can cause obfuscate the results and limit the user's ability to select them. As an initial test, node sizes can be reduced. However, this may often result in points too small to pick. Another solution is to create a force-directed graph where each point is strongly attracted to its initial position. Here we can apply collision detection, while still preserving the overall grouping of nodes within a cluster - a technique that was seen in \autoref{ch4}.


\subsection{Gooey Effect (Gaussian Blur)}
Taking a quote from \cite{lessmore}:
\emph{``The more stuff in it, the busier the work of art, the worse it is. More is less. Less is more. %The eye is a menace to clear sight.
''} and combining it with the work from \autoref{ch1}, we realise that showing each species, when observing overall clusters just add unnecessary clutter to the images. Instead, since we are only interested in the clusters as a unit, a `gooey effect' filter can be applied. This works by merging nearby points into a single water-like blob using a gaussian blur\footnote{Here a gaussian blur of standard deviation 3.7 and a colour matrix [1 0 0 0 0  0 1 0 0 0  0 0 1 0 0  0 0 0 37 -5] is used.}. Here since each point is allocated a colour, if a colour gradient exists, then there are multiple clusters occupying the same place. The aim of this is to reduce the cognitive load on the end-user by reducing the number of distinct objects that they need to take in.



\subsection{Four Colours Theorem}\label{sec:4col}
When plotted, the number of clusters detected often exceeds the number of categorical colours available. In cartography, it has been noted that the colouring of neighbouring polygons should at most take four colours. This is the origin of the four colours theorem \citep{fourcolour}, of which a greedy implementation is applied.

The aim of this is to show item boundaries (for instance countries, or in our case clusters) while reducing ambiguity (if, say, two neighbours have the same colour). The algorithm uses the Delaunay tesselation from DataDrivenDocuments.js (d3js) \citep{d3js}. This partitions our plane into polygon-regions, each of which includes boundaries at the furthest distance from each point (Voronoi cells) \citep{delaunay}. First, we chose a random cell and assigned it a colour.
Next, all its neighbours are recursively iterated, giving them the lowest possible colour in a list, which does not match any of their neighbours. Although such a greedy approach does not produce an optimum result, it allows for the colouring of data with $\le 5$ distinct colours, as is shown in \autoref{fig:4col}.


\begin{figure}[H]
  \centering
\includegraphics[width=.8\textwidth]{4fig/4col.png}
\caption{\textbf{An example 4 colour matching} This uses the first implementation of the algorithm mentioned in \autoref{sec:4col}. The greedy approach does not often find the optimum solution, which may result in 5 colours instead. Observable Notebook : \cite{w4colobs}}\label{fig:4col}
\end{figure}


Having defined all the visualisation techniques we can now move on to explain the clustering algorithms which are used, and how `goodness of fit' may be measured in the clustering context.

\section{Cluster Evaluation}
The previous section discussed methods of visualising the reduced data for use with interactive exploratory data analysis. In this section we look at the use of vector clustering algorithms\footnote{Vector clustering is the grouping of data based on their proximity or density to other nearby points} (\autoref{sec:drclustt}) to highlight groups in a 2D dataset, as well an automated method of assessing the quality of the clusters selected (\autoref{sec:silo}) and feature extraction (\autoref{sec:drfeature}).



\subsection{Automated Selection Of Clusters}\label{sec:drclustt}
    When it comes to clustering data points in a dataset, there exist a range of methods which may accomplish a task, \autoref{fig:clustereval}. Most often, the k-means \citep{kmeans} is used as it is fast and straightforward to understand. However, its linear method of partitioning cannot capture the splits between non-linear relationships of real data. The other problem is that an estimate for the number of expected clusters is required - something that is often unknown without prior understanding of the data. When this is the case, often it is easier to select the nodes with interactivity manually.

In contrast, density-based clustering techniques such as GMM (\citep{scikit}) or DBSCAN (\citep{DBSCAN}) tend to be better at locating non-linear trends in the data. The DBSCAN algorithm asses the distribution of data across a specific location. This allows clusters with a high density of datapoints to be located without the need for a predefined number as an input. Another method: OPTICS (Ordering Points To Identify the Clustering Structure) \citep{optics}, shall be used\footnote{ If using Python 2, the library for this needs to be extracted from the sci-kit-learn library for python3 package and altered to run with the previous version. (See copy in attached code.)}. This is an adaptation of the DBSCAN algorithm which does not require the specification of a minimum distance between points (for the density estimate)- instead, we specify a gradient for the distribution and the minimum number of points for a cluster to be classified.


\begin{figure}[H]
     \centering

         \includegraphics[width=\textwidth]{4fig/clustereval.png}

        \caption{\textbf{A comparison of different clustering methods on a toy dataset.} The plot shows the performance of several vector clustering algorithms in Scikit-Learn. Cluster algorithms are represented across the horizontal axis, and several types of datasets are across the vertical. Clustered groups are coloured. Source: \citep{clustereval}}
        \label{fig:clustereval}
\end{figure}


When deciding which algorithms to use, each algorithms' ability to partition non-linear data is considered.
The first two rows of \autoref{fig:clustereval} show data which cannot be partitioned linearly, here spectral, DBSCAN and optics are the only clustering algorithms to identify both correctly. It is for this reason that we shall look at these for the remainder of the chapter.

In selecting a value for the results section, several clustering algorithms, with a wide range of input parameters, are run. From these, the simulation with the best silhouette coefficient (\autoref{sec:silo}) is taken.




\subsubsection{Clustering (Silhouette) Coefficient}\label{sec:silo}
The silhouette measure is a tool used for acessing the validity of a set of clusters. Here each cluster is represented as a silhouette, based on the comparison of its tightness and separation. To calculate the silhoette coefficient we look at the intra-cluster $a$ and the mean inter-cluster\footnote{Inside and between different clusters.} distance $b$. The silhouette cluster can then be described using \citep{silhouette,sklearn}:

\begin{equation}
s(i) = \frac{b(i)-a(i)}{\max{ a(i), b(i)}}
\end{equation}

This gives a value $-1 \le s(i) \le 1$. Values near zero suggest overlapping clusters, 1 - dense, well-separated clusters and negative values indicate that a sample may have been incorrectly classified. In using this method, we can get an overview of how well individual objects lie within their assigned cluster.




\subsection{Feature Extraction}\label{sec:drfeature}
Upon establishing a set of DR datasets, and their groups (the clusters of species they contain), it is important to evaluate what input features they represent. Rather than doing this manually, we make use of Random Forests - described below.

\subsubsection{Random Forrests}
Random forests \citep{rfrr}, are a subset of ML algorithms called ensemble learning. This means that they train a large number of decision trees, each on a random subset of the original features. A decision tree is a tree formed from a series of conditionals\footnote{Questions with a True/False answer}, much like a perceptron network (\autoref{sec:perceptron}) with binary activation functions. Random forests introduce a level of additional randomness by selecting only a subset on which to create each decision tree. This may introduce a higher bias, but lowers the overall model variance, which creates a better (more robust) model. Such methods have been applied to replacing the computationally expensive process of chemistry integration of GEOS-Chem (a global 3D model of tropospheric chemistry) \citep{geosrf} and the prediction of global sea-surface iodine based on observations coupled with sea-surface temperature, depth, and salinity \citep{iodene}.

\subsubsection{Calculating Importance Using Random Forrests}
Since random forests are in essence a collection of decision trees, it is possible to generate a `decision tree aggregate' to visualise the ensemble structure of the random forest \citep{forrester} (\autoref{fig:iodenetree}). Alternatively, if all that is required is the relative importance of each feature, the RandomForrestClassifier from \citep{sklearn} provides a quick and easy way of understanding which features matter,
\citep{handsonml}. This works by aggregating the weighted nodes which use a particular feature using the number of samples and then scales the result to 1. We use this method to access the overall importance of features within each DR output and identify the differences between clusters.


\begin{figure}[H]
     \centering
         \includegraphics[width=.55\textwidth]{4fig/Oi_prj_features_of_RFR(TEMP+DEPTH+ChlrA)_for_depth_5_white.pdf}
        \caption{\textbf{A decision tree aggregate from a random forest plotted with the Epiphyte version of the TreeSurgeon program \citep{forrester}.} The data originates from \cite{iodene} and the importance of Temperature (blue), Depth (orange) and Chlorophyll $a$ (green). It is shown that all models create their first split based on the temperature (is it $>$21 degrees). In the case it is (right branch) the sea depth is seen as the most important variable to test (is it deeper than 26m). This sort of split allows us to get a feel for which (if any) properties are dominant in partitioning the data. }
        \label{fig:iodenetree}
\end{figure}


%

\textit{NOTE: The only downside is that Random Forrests are in themselves ML techniques which also need to be evaluated. To do this, as they are simply being used as indicators of cluster properties which we are to explore further, we can initiate a collection of 300 random Forrest classifiers, from which we take the median. A sort of ensemble learning from an ensemble. }


\section{Results}\label{sec:drres}
%
There exist many methods of defining the chemical structure of species within the MCM. This section evaluates the different structural representations (\autoref{sec:drinput}) and the ability of DR algorithms to separate the chemical space within which these lie into a two-dimensional scatterplot.

%

\subsection{Visual Overview}\label{sec:cldist}

Explorative data analysis involves a degree of figure interactivity. \autoref{ch1} described the importance of visualisation in employing the cognitive pattern functions of the human brain, and \autoref{ch2} explained the importance of having an evenly distributed data points to aid the understanding of graphs. This subsection combines the two ideas in the usage of dimensionality reduction to exploit patterns within a dataset.

Using the techniques in \autoref{sec:drvis}, we explore the visual distribution of different dimensionality reduction methods across all input types. This subsection explores the spatial distribution of groups (blobs) in the 2D dimensionally reduced dataset. The colours represent the automatically calculated clusters (which are further explored analytically in \autoref{sec:mathclustanalysis}).
This is then built upon using three case studies, where individual cluster distributions are compared (\autoref{sec:fsclust}).


An autoencoder will produce near-identical results to the PCA algorithm, using linear activation functions. Them ain difference is that rather than discarding components which represent little variance, the neural network (autoencoder) combines their values when deciding on the output. Although ordinal data (\autoref{fig:aevis} b,f) still produce regular patterns, the non-linear (tanh) activation functions result in a greater separation between data points for the SMILES dataset (\autoref{fig:aevis}g).

Unlike PCA and AE, t-SNE does not contain any inherent meaning behind the spatial positioning of points. Instead, it provides a non-linear grouping of points through a graph-like force-directed model (all items are connected with the weights of the links decided by their relationship values). This results in the most visually pleasing output die to clusters increased separation. This property also makes it the easiest to visually isolate clusters from their neighbours (\autoref{fig:tsnevis}), making it a useful tool for interactive data exploration or explaining groups within a figure.


\begin{landscape}
\begin{figure}[H]
    \subimport{tables/}{pcadr.tex}
    \caption{\textbf{Comparing clusters for all inputs after a reduction to 2 dimensions using Principal Component analysis.}
    Each graph has undergone several clustering algorithms under a range of parameters. The result with the best silhouette coefficient has been chosen. Colours follow the greedy four colour theorem and are there only to indicate the contrast between cluster boundaries.}
    \label{fig:pcavis}
\end{figure}
\end{landscape}


\begin{landscape}
\begin{figure}[H]
    \subimport{tables/}{aedr.tex}
    \caption{\textbf{Comparing clusters for all inputs after a reduction to 2 dimensions using an AutoEncoder.}
    Each graph has undergone several clustering algorithms under a range of parameters. The result with the best silhouette coefficient has been chosen. Colours follow the greedy four colour theorem and are there only to indicate the contrast between cluster boundaries.}
    \label{fig:aevis}
\end{figure}
\end{landscape}


\begin{landscape}
\begin{figure}[H]
    \subimport{tables/}{tsnedr.tex}
    \caption{\textbf{Comparing clusters for all inputs after a reduction to 2 dimensions using t-SNE.}
    Each graph has undergone several clustering algorithms under a range of parameters. The result with the best silhouette coefficient has been chosen. Colours follow the greedy four colour theorem and are there only to indicate the contrast between cluster boundaries.}
    \label{fig:tsnevis}
\end{figure}
\end{landscape}



\subsection{Mathematical Cluster Analysis}\label{sec:mathclustanalysis}

\autoref{sec:cldist} explored the visual appeal of the plotted clusters. For large systems, the selection of many clustering algorithm outputs is impractical, so the analysis of DR methods has been automated (\autoref{sec:drclustt}). Similarly, we can once again apply the silhouette coefficient (\autoref{sec:silo})
to compare the best output for each input and DR algorithm.

Outputs for the number of groups the DR output has been clustered in, and its corresponding silhouette coefficient are shown in Tables \ref{tab:pcasil} - \ref{tab:tsnesil}. These shall be discussed by name rather than being referenced each time for ease in reading.
Inputs for each algorithm are ranked in order of their silhouette coefficient (the closer to 1 the value, the better the clustering).

Ordinal inputs such as functional groups or the protocol categories consistently rank the highest within each algorithm. This is because the algorithms only have to identify the permutations of each category and classify the species into these. It is noted that the t-SNE silhouette coefficient for these is ~30\% lower than for PCA and AE algorithms. However, the number of groups located is also greatly reduced from 140 to 106. This suggests that the vector clustering algorithms have tried to combine data points into a group, which has come at a cost to the silhouette value.

Next, the SMILES strings are ranked highly for both AE and t-SNE algorithms. Visually this agrees with \autoref{fig:aevis}(g) and \autoref{fig:tsnevis}(g), where these are much better separated than \autoref{fig:pcavis}(g). The PCA ranking is almost half of other algorithms, but contains a much smaller number of `clusters'. This can be attributed to its periodic lattice-like spacing of points which are not conducive to producing good vector clustering groups in an unsupervised algorithm.

MACCS keys, although providing a modest silhouette score across most DR algorithms, often only have two or three clusters. The reason for this is that dimensionality reduction of these often separate species into two groups, those with Nitrogen elements and those without (\autoref{fig:maccsmath}). This is because many of the questions on which the MACCS fingerprint is based concern themselves with Nitrogen species \citep{rdkitcode}. Group composition and cluster analysis is discussed in \autoref{sec:fsclust} and \autoref{sec:selectcomp}.\\

\begin{figure}[H]
    \includegraphics[width=1.3\textwidth]{outputs/PCA/maccs/group.png}
    \caption{\textbf{The individual decomposition of clusters in \autoref{fig:pcavis}(c)}This shows that the main difference between the two clusters is the existence of Nitrogen elements within Nitrate and Peroxyacetyl Nitrate (PAN) groups. The table on the right acts as a key for the colours and shows the overall importance of each feature in separating an item into the various clusters (using an ensemble of decision trees).}
    \label{fig:maccsmath}
\end{figure}

Similar to SMILES strings molecular quantum numbers result in plots consisting of regular rows of data with like-properties (\autoref{fig:pcavis}-\ref{fig:tsnevis}(d)) - making it difficult for the clustering algorithms to select each group correctly. This property makes it suitable for usage in the field of chemical informatics where molecules with similar properties are desired \citep{mqnpca}, but less so for establishing an overarching categorisation of species within a mechanism.

Finally, the graph-based fingerprint input is consistently the lowest scoring in the silhouette coefficient. Visually this can be attributed to the mismatching of cluster number to the number of visually separate clusters  (\autoref{fig:pcavis}-\ref{fig:tsnevis}(a)). The random distribution of these visual `specs' suggests that the custom graph-fingerprint is not the most informative input for use with DR and clustering algorithms.

Overall combining the visual representation of selected clusters (\autoref{sec:cldist}) with the silhouette scores, show that the clustering algorithms struggle with the prediction of correct groups. Although the silhouette coefficient is a useful metric for determining the density of points around a cluster, it should be used with the aid of other metrics if selecting the best results from an automated clustering process. Additionally, the automatic clustering process can include feedback from the goodness of fit metrics, which can allow for the finer tuning of clustering algorithm parameters in future.


\begin{table}[H]
    \centering
        \subimport{tables/}{silhouettepca.tex}
        \caption{The inputs to the PCA dimensionality reduction algorithm sorted by the best obtained silhoette coefficient.  }
        \label{tab:pcasil}
\end{table}


\begin{table}[H]
    \centering
        \subimport{tables/}{silhouetteae.tex}
        \caption{The inputs to the AutoEncoder dimensionality reduction algorithm sorted by the best obtained silhoette coefficient.  }
        \label{tab:aesil}
\end{table}

\begin{table}[H]
    \centering
        \subimport{tables/}{silhouettetsne.tex}
        \caption{The inputs to the t-SNE dimensionality reduction algorithm sorted by the best obtained silhoette coefficient.  }
        \label{tab:tsnesil}
\end{table}









\subsection{Feature Selection Comparison}\label{sec:fsclust}

The previous subsection assessed how well DR algorithms were able to separate the chemistry of a mechanism into distinct, well-defined clusters. Now the content of each of those groups is looked at, comparing them to the functional groups most responsible for the variation within the 2D compression of a chemical mechanism. Importance of each functional group in explaining cluster composition is obtained using a random forest classifier (Figures \ref{fig:pcalegend}-\ref{fig:tsnelegend}).

Comparing all three DR algorithms, we see that the shape (and this group importance) is persistent between each input across all algorithms. This means that although values may differ, the same functional groups are important between each DR algorithm - indicating that this is a property of the input style and not the type of dimensionality reduction technique selected.

In establishing that the input format is responsible for the splitting of clusters into groups, we look at what the relationship of these is. In this section, the characters (a-h) refer to \textbf{all} the corresponding subplots in Figures \ref{fig:pcalegend}-\ref{fig:tsnelegend}.

 In the Common Representative Intermediates (CRI) mechanism, the ratio of C-H and C-O bonds are used to lump species with the same oxidation capacity \citep{cri}. This makes the ratio of carbons to oxygens an essential defining characteristic between them. The number of Oxygens and Carbons is a consistently important feature for all input styles (with the exception of the gecko protocol categories (f) ). Here the number of carbons or oxygens does not fit any of the reaction branches meaning that there is no way for the dimensionality reduction algorithm to know these. Instead, Alcohol and Carbonyl groups are seen as the most important in separating the chemistry, which may be due to the number of species undergoing each type of reaction.

 Species names (h) have prefixes (e.g. C$xx$) and suffices (e.g., -OOH, -\ch{NO3}, -\ch{O2}, -OL) which allow an easy way for a user to distinguish the types of species, but also the DR algorithms. These show the second-best separation of aromatic species, most likely attributed to the standard naming convention, e.g. (`BENZ', `PIN', `TMB'). Similarly, the SMILES string (g) shows a human-readable representation of the structure of the molecule. Since this is explicitly defined, the SMILES input provides the highest uniformity in group types when analysing cluster composition. As aromatic compounds are represented using a lower case `c', this makes them easy to distinguish (especially in the case of AE \autoref{fig:aelegend}g).

 As was touched on in \autoref{sec:mathclustanalysis} the MACCS input consists of a series of logical questions about a species structure. Since many of those questions regard the existence of a Nitrogen atom, data was separated species with a Nitrate or PAN group, and those without. In making a series of decisions on which cluster a species falls under, this largest most recurring branch for the RandomForrestClassifier (imagine of temperature in \autoref{fig:iodenetree}) falls under the existence of a Nitrate group.

 The main inconsistency between clusters and DR algorithms comes from the node2vec embedding (e) - much of which can be explained by the poor performance of the DR and clustering algorithms of separating the chemistry into groups (see plots in \autoref{sec:cldist}). \autoref{sec:selectcomp} continues this analysis by comparing output with $<3$ clusters each against the graph plots presented in this subsection. The content of individual groupings is explored for an output with multiple clusters.


\begin{figure}[H]
    \subimport{tables/}{pcalegend.tex}
    \caption{\textbf{Comparing feature importance for PCA clusters.} Importance ranges are trimmed at 40\% for comparison. Some categories may contain values greater than this. All bars sum to 100\%.}
    \label{fig:pcalegend}
\end{figure}
\begin{figure}[H]
    \subimport{tables/}{aelegend.tex}
    \caption{\textbf{Comparing feature importance for AE clusters.} Importance ranges are trimmed at 40\% for comparison. Some categories may contain values greater than this. All bars sum to 100\%. }
    \label{fig:aelegend}
\end{figure}
\begin{figure}[H]
    \subimport{tables/}{tsnelegend.tex}
    \caption{\textbf{Comparing feature importance for t-SNE clusters.} Importance ranges are trimmed at 40\% for comparison. Some categories may contain values greater than this. All bars sum to 100\%.}
    \label{fig:tsnelegend}
\end{figure}


\subsection{Cluster Comparison}\label{sec:selectcomp}

In this final subsection, we look at the composition of different clusters within the dimensionally reduced dataset. We begin by looking at the simplest cells from \autoref{sec:cldist} - ones which only contain two or three cluster groups (\autoref{sec:bigroups}) and then move on to explore three examples showing multiple clusters (\autoref{sec:multigroups}).



\subsection{Bi / Tri Cluster Groups } \label{sec:bigroups}

Using the DR output where only two/three groups are located by the clustering algorithms we have (\autoref{fig:biMACCS} and \autoref{fig:biN2V}). In exploring the MACCS key input for the PCA and t-SNE DR algorithms (\autoref{fig:biMACCS}) we find that for the cumulative importance bar charts we know that the existence of Nitrates is vital in the split determining which group a species falls into. This manifests itself as having a single cluster containing PAN and Nitrate species, with others not. In the t-SNE plot (\autoref{fig:biMACCS}b) we see that there exists a third group which is missing both Aldehyde and PAN functionalisation for each species. This is shown by the teal colour in \autoref{fig:tsnevis}c and resides between the Nitrogen-containing and Nitrogen-deficient groups.

\autoref{fig:biN2V} shows the comparison of the Node2Vec embedding using PCA and the AE DR algorithms. In \autoref{fig:pcavis}e and \autoref{fig:aevis}e, it is seen that these are generally not separated into well-partitioned clusters. Both groups consist of one large cluster (shown by the second bar chart of each row which contains all functional groups) and one or two fragment ones. In exploring the AE plot (\autoref{fig:biN2V}b), it is seen that as part of the cumulative plot (right), the -OOH functional group is an important separatory factor since the smaller of the two groups does not contain any species which contain a hydroperoxy functional group. In the PCA plot, although providing different cumulative results, again shows species within the smaller groups not containing any \ce{RO, RCO3, OOH, ONO2} or OOH functional groups. This can potentially be due to the graph structure, where the random walker (which generates the node2vec embedding) has become trapped by a group of non-oxidised species.


\begin{landscape}
\begin{figure}[H]

         \includegraphics[width=1.6\textheight]{./outputs/PCA/maccs/group.png}
         \\ (a) PCA \\
     \hfill
            \includegraphics[width=1.6\textheight]{./outputs/t-SNE/maccs/group.png}
        \\ (b) t-SNE
        \caption{ \textbf{Comparing individual clusters between MACCS for PCA and t-SNE algorithm output.} The bar chart to the right is the cumelative chart which represents the splits in deciding the cluster a species falls into from \autoref{sec:fsclust}. Unlabeled bar charts to the left represent the partitioning of species within an individual cluster.}
        \label{fig:biMACCS}
\end{figure}
\end{landscape}



\begin{landscape}
\begin{figure}[H]
         \includegraphics[width=1.6\textheight]{./outputs/PCA/node2vec/group.png}
         \\ (a) PCA \\
     \hfill
            \includegraphics[width=1.6\textheight]{./outputs/AE/node2vec/group.png}
        \\ (b) AE
        \caption{ \textbf{Comparing individual clusters between node2vec for PCA and t-SNE algorithm output.} The bar chart to the right is the cumelative chart which represents the splits in deciding the cluster a species falls into from \autoref{sec:fsclust}. Unlabeled bar charts to the left represent the partitioning of species within an individual cluster.}
        \label{fig:biN2V}
\end{figure}
\end{landscape}



\subsubsection{Multicluster Groups}\label{sec:multigroups}

Next, we observe several multi clustered examples from all DR algorithms. We start with \autoref{fig:pcagraphcase} where the PCA algorithm has generated an extensive collection of points in an area. Unsurprisingly the clustering algorithm has failed to identify separate groups (as there is only one), and instead partitioned the data into six groups. Although not ideal, this still has its use in determining how species have been partitioned temporally. Here we find that nitrogen-containing species are positioned on the right side of the graph, and aromatic species are in the bottom half. \ce{RO2} containing species span the entirety of \autoref{fig:pcagraphcase}, but increase towards the left direction. This shows that although PCA was unable to separate the chemical species of the MCM into groups from the fingerprint input, it still presents patterns within the data such that the arrangement of groups can be seen when querying specific areas.





\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/PCA/fingerprints_all.pdf}
    \caption{\textbf{Case Study 1: PCA graph-fingerprint} We compare the functional group distribution for individual clusters within the PCA 2D representation of the graph-fingerprint input.}
    \label{fig:pcagraphcase}
\end{figure}

\newpage

Next, we explore the AE algorithm. Here the smiles input separates the data into more defined groups. This becomes apparent with specific functional groups only appearing within individual clusters (\autoref{fig:aesmilescase}) - as opposed to the gradients seen in \autoref{fig:pcagraphcase}. Here all the aromatic species are contained within the central (pink cluster). Similarly, the green and brown clusters do not contain any nitrates, and most of the PAN species are within the grey cluster in the top right. The better separation of clusters aids in the identification of groups (by the automated vector clustering algorithm) as well as highlighting the process undergone within the DR algorithm to partition the data.






\begin{figure}[H]
    \centering
    \includegraphics[height=\textwidth]{outputs/AE/smiles_all.pdf}
    \caption{\textbf{Case Study 1: AE SMILES} We compare the functional group distribution for individual clusters within the AE 2D representation of the SMILES input.}
    \label{fig:aesmilescase}
\end{figure}
\newpage

Finally, we explore the t-SNE DR algorithm using the MQN inputs (\autoref{fig:tsnemqncase}). Previously it is found that the t-SNE produces the most well-defined clusters, the cost of which comes from losing information about group similarity encoded in the distance separating them. Here \ce{RCO3} containing species are located in the central turquoise cluster, PAN species in the top right (lime) cluster and the aromatic species are all in the pink cluster at the bottom.
The brown and purple clusters (top left) contain no carbonyl groups, and the purple, grey (top left) and orange (mid-right) contain not Nitrates. The t-SNE provides the best spatial distribution of groups. However, inspecting cluster colours visually suggests that the automatical vector clustering algorithm has not necessarily located the best combination of groups. As was suggested in \autoref{sec:mathclustanalysis} a more dynamic method of tuning the clustering algorithm hyperparameters may result in better cluster selection - which may better separate each cluster's species category.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/t-SNE/mqn_all.pdf}
    \caption{\textbf{Case Study 1: t-SNE MQN.} We compare the functional group distribution for individual clusters within the t-SNE 2D representation of the Mollecular Quantum Number fingerprint.}
    \label{fig:tsnemqncase}
\end{figure}
\newpage

\section{Conclusions}

This chapter aims to tie up the research presented in \autoref{ch1}-\ref{ch4} in preparation for future using Graph Convoluted Neural Networks \citep{gcn} to classify and even predict (generate) mechanisms (or at least attempt to). In \autoref{ch2} we showed that networks are a useful representation for the relational nature of species within the atmosphere, and then applied several mathematical techniques to it \autoref{ch3}-\ref{ch4}. This chapter looked at simplifying chemical structure used by many dimensionality reduction algorithms and using visualisation and computational algorithms to assess their ability at partitioning species into similar groups.

It was found that t-Distributed Stochastic Neighbor Embedding (a graph-based DR technique) provided the best results for species separation and visualisation. This provides a non-linear mapping of the relationships between items and does not omit any data (PCA) or require the selection of layers and activation functions (AE). It does, however, lose information about cluster similarity based on distance and cannot be used to compress (encode/decode) information - although neither of those features are required for our use.

Additionally, several possible inputs for each dimensionality reduction algorithm were used. Since machines cannot understand the meaning of words, these are different representations of species structure we can put into a machine-learning algorithm to inform it of a species. Out of the non-ordinal inputs, it was found that tokenised SMILES strings and the Molecular Quantum Number fingerprint produced 2D visualisations with the best separation between clusters. Other inputs either caused large groups of overlapping nodes or had a strong dependency on a single species or functional group (e.g. MACCS and Nitrates).

It is suggested that within future work, the t-SNE dimensionality reduction algorithm is used if trying to visualise the different groups of a complex dataset. In the case of any machine learning algorithms, the MQN fingerprint should be used for generic examples, unless a specific feature is required from the input. If instead a more complex/larger input is required, it is also possible to apply any of the DR algorithms discussed to simplify it. Here the use of an AutoEncoder is suggested as the encoder-decoder pair allows for the testing of a trained model (as well as the ability to explore the embedded space). Although this can be achieved through the use of principal component analysis, this does not handle non-linear relationships and cannot be trained on a further dataset if one becomes available.
